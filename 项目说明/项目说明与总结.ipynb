{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=black size=6 face=\"微软雅黑\"><b> 天池阿里云安全恶意程序检测 </b></font>  ------<font color=black size=6 face=\"微软雅黑\"> 刘茂毅 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 比赛说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 赛题背景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恶意软件是一种被设计用来对目标计算机造成破坏或者占用目标计算机资源的软件，传统的恶意软件包括蠕虫、木马等，这些恶意软件严重侵犯用户合法权益，甚至将为用户及他人带来巨大的经济或其他形式的利益损失。近年来随着虚拟货币进入大众视野，挖矿类的恶意程序也开始大量涌现，黑客通过入侵恶意挖矿程序获取巨额收益。当前恶意软件的检测技术主要有特征码检测、行为检测和启发式检测等，配合使用机器学习可以在一定程度上提高泛化能力，提升恶意样本的识别率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 赛题说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T02:32:18.158079Z",
     "start_time": "2019-01-10T02:32:18.132077Z"
    }
   },
   "source": [
    "本题目提供的数据来自文件（windows 可执行程序）经过沙箱程序模拟运行后的API指令序列，全为windows二进制可执行程序，经过脱敏处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本题目提供的样本数据均来自于从互联网。其中恶意文件的类型有感染型病毒、木马程序、挖矿程序、DDOS木马、勒索病毒等，数据总计6亿条。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1）训练数据（train.zip）：调用记录近9000万次，文件1万多个（以文件编号汇总），字段描述如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 字段 | 类型 | 解释 |\n",
    "| --- | --- | --- |\n",
    "| file_id | bigint | 文件编号 |\n",
    "| label | bigint | 文件标签，0-正常/1-勒索病毒/2-挖矿程序/3-DDoS木马/4-蠕虫病毒/5-感染型病毒/6-后门程序/7-木马程序 |\n",
    "| api | string | 文件调用的API名称 |\n",
    "| tid | bigint | 调用API的线程编号 |\n",
    "| index | string | 线程中API调用的顺序编号 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T02:42:36.648454Z",
     "start_time": "2019-01-10T02:42:36.610452Z"
    }
   },
   "source": [
    "注1：一个文件调用的api数量有可能很多，对于一个tid中调用超过5000个api的文件，我们进行了截断，按照顺序保留了每个tid前5000个api的记录。<br>\n",
    "注2：不同线程tid之间没有顺序关系，同一个tid里的index由小到大代表调用的先后顺序关系。<br>\n",
    "注3：index是单个文件在沙箱执行时的全局顺序，由于沙箱执行时间有精度限制，所以会出现一个index上出现同线程或者不同线程都在执行多次api的情况，可以保证同tid内部的顺序，但不保证连续。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2）测试数据（test.zip）：调用记录近8000万次，文件1万多个。<br>\n",
    "说明：格式除了没有label字段，其他数据规格与训练数据一致。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T02:46:22.218356Z",
     "start_time": "2019-01-10T02:46:22.204356Z"
    }
   },
   "source": [
    "## 评测指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.选手的结果文件包含9个字段：file_id(bigint)、和八个分类的预测概率prob0, prob1, prob2, prob3, prob4, prob5 ,prob6,prob7 (类型double，范围在[0,1]之间，精度保留小数点后5位，prob<=0.0我们会替换为1e-6，prob>=1.0我们会替换为1.0-1e-6)。选手必须保证每一行的|prob0+prob1+prob2+prob3+prob4+prob5+prob6+prob7-1.0|<1e-6，且将列名按如下顺序写入提交结果文件的第一行，作为表头：file_id,prob0,prob1,prob2,prob3,prob4,prob5,prob6,prob7。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.分数采用logloss计算公式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ logloss=-\\frac{1}{N}\\sum_i^N\\sum_j^M\\left \\lfloor y_{ij}log\\left ( P_{ij} \\right ) + \\left ( 1-y_{ij} \\right )log\\left ( 1-P_{ij} \\right )  \\right \\rfloor $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M代表分类数，N代表测试集样本数，yij代表第i个样本是否为类别j(是~1，否~0)，Pij代表选手提交的第i个样本被预测为类别j的概率(prob)，最终公布的logloss保留小数点后6位。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方案说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 赛题分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 这是一个多分类问题，种类为8\n",
    "* 数据分析\n",
    "    * 官方给定数据集中：单个文件(file_id)对应多条数据，需要重新处理，使得一个文件(file_id)对应一条数据一个label\n",
    "    * 文件(file_id)经过沙箱后，产生多个线程(tid)，每个线程(tid)中有很多API调用(关键在顺序和个数)\n",
    "        * 考虑到沙箱执行时有精度限制，所以要在同一个线程中(tid)合并连续相同API    \n",
    "* 数据特别点\n",
    "    * data中的API种类一共有295个，test中的API种类一共有298个，总共有301种，在构建特征时需要包含所有API种类\n",
    "* 特征构建时，可以用bag of word 、tf-idf、N-gram、Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征和模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW+LR/DNN/Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train和test量比较大，对train和test中的API列进行LabelEncoder，删除原有API列，新增LabelEncoder列，存入pickle<br>\n",
    "* 数据预处理<br>\n",
    "   * 统计每个file_id对应的线程总数、API总数、每个API的调用次数<br>\n",
    "   * 特征列如下：<img src=\"./pictures/feature.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型 LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 对数据做标准化后，放入LR中<br>\n",
    "* 提交结果 <img src=\"./pictures/rank1.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型 DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数据做标准化后，放入DNN中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=feature_dim))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "# model.add(Dense(64, activation='sigmoid', input_dim=feature_dim))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "# model.add(Dense(8, activation='softmax', input_dim=feature_dim))\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <img src=\"./pictures/nn.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 提交结果 <img src=\"./pictures/nn_results.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型 Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征直接放入Xgboost中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 提交结果 <img src=\"./pictures/xgboost.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T07:12:42.775392Z",
     "start_time": "2019-01-10T07:12:42.740390Z"
    }
   },
   "source": [
    "* 同一特征，模型xgboost效果最好\n",
    "* 当然，DNN的参数可以再调一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW+TF-IDF + xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在BOW的基础上，加上TF-IDF（文档库是train和test）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型仍然用xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是，加上tf-idf后，模型效果和不加的时候差不多，故没有提交该模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-gram + xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train和test量比较大，对train和test中的API列进行LabelEncoder，删除原有API列，新增LabelEncoder列，存入pickle\n",
    "* 按照顺序，把同一文件中的API调用按照顺序组成list，合并同一个文件(file_id)、同一线程(tid)中相邻相同API\n",
    "* 2-gram构建特征。选择2是因为，再高维度服务器处理不了。。\n",
    "    * vec = CountVectorizer(min_df=1, ngram_range=(2,2))\n",
    "    * 注意 fit的时候要包括data和test中的所有api\n",
    "    * X=vec.fit(list_file_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造好的特征放入模型xgboost中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数设定\n",
    "param2 = {'max_depth':6, 'eta':0.5, 'silent':1, 'subsample':0.7, 'gamma':0.2,'colsample_bytree':0.7, 'objective':'multi:softprob',\\\n",
    "         \"num_class\":8 }\n",
    "# 设定watchlist用于查看模型状态\n",
    "# watchlist  = [(xgtest,'eval'), (xgtrain,'train')]\n",
    "watchlist  = [(xgtrain,'train')]\n",
    "num_round = 25\n",
    "bst2 = xgb.train(param2, xgtrain, num_round, watchlist,early_stopping_rounds=500)\n",
    "print(\"最佳决策树数量：\",bst2.best_ntree_limit)\n",
    "print(\"模型最佳分数：\",bst2.best_score)\n",
    "print(\"最佳迭代次数：\",bst2.best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 提交结果 <img src=\"./pictures/n-gram-xgboost.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW+TF-IDF + N-gram + xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把BOW+TF-IDF的特征和N-gram的特征合并到一起，concate即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把合并后的特征放到xgboost中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 提交结果 <img src=\"./pictures/ngram-wordnum-tfidf.png\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T06:29:23.671732Z",
     "start_time": "2019-01-10T06:29:23.650730Z"
    }
   },
   "source": [
    "### Embedding+TextCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 按照顺序，把同一文件中的API调用按照顺序，用空格分开，组成新的字符串，并合并同一个文件(file_id)中相邻的API。\n",
    "* 这样，每个文件对应一个长字符串\n",
    "* 把这些字符串放到list中\n",
    "* 把list转成pad_sequence，最多存7000个word，不够的7000的，后面补齐0，超过7000的，截断\n",
    "* 存成pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把特征放到模型TextCNN中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 64\n",
    "batch_size= 32\n",
    "cnn_filters = 256\n",
    "dense_hidden_dims = 256\n",
    "_kernel_size = [3]\n",
    "\n",
    "def get_model_function():\n",
    "    main_input = Input(shape=(maxlen,), dtype='float64')\n",
    "    _embed = Embedding(304, embed_dim, input_length=maxlen)(main_input)\n",
    "    _embed = SpatialDropout1D(0.5)(_embed)\n",
    "    warppers = []\n",
    "    conv_action = 'relu'\n",
    "    \n",
    "    conv1d = Conv1D(filters=cnn_filters, kernel_size=_kernel_size, activation=conv_action)(_embed)\n",
    "    conv1_2 = Conv1D(cnn_filters, 3, padding='same')(conv1d)\n",
    "\n",
    "    fc=GlobalMaxPooling1D()(conv1_2)\n",
    "    fc = Dropout(0.2)(fc)\n",
    "    fc = Dense(dense_hidden_dims, activation='relu')(fc)\n",
    "\n",
    "    fc = Dropout(0.5)(fc)\n",
    "    preds = Dense(8, activation='softmax')(fc)\n",
    "    model = Model(inputs=main_input, outputs=preds)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 5-折交叉验证\n",
    "* 得到预测后的train和test\n",
    "* 计算train对应的logloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "天池测评机坏了，提交不了，故该模型没有提交结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4个（TextCNN） 和 1个（BOW+TF-IDF + N-gram + xgboost）做加权平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TextCNN: 取_kernel_size=3/7/9/13，单模型上trian数据集对应的 logloss分别为-0.59、-0.59、-0.60、-0.60\n",
    "    * 做加权平均后logloss为0.53\n",
    "* BOW+TF-IDF + N-gram + xgboost五折交叉验证，单模型上train数据集对应的logloss为0.52\n",
    "* 以上两个做加权平均，权重为0.5和0.5，融合后train数据集对应的logloss为0.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "天池测评机坏了，提交不了结果，故该模型没有提交结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最后名次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如图："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pictures/rank_2019-01-11.png\" width=\"90%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
